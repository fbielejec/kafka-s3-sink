#+TITLE: kafka-demo

* Prerequisites
Install confluent hub client:

#+BEGIN_SRC bash
curl -O http://client.hub.confluent.io/confluent-hub-client-latest.tar.gz
# add to .bashrc:
export PATH="<path-to>/confluent-hub-client-latest/bin:$PATH"
# test
confluent-hub help
#+END_SRC

* Development

Start docker containers:

#+BEGIN_SRC bash
docker-compose -f docker-compose.yml up
#+END_SRC

Start message produces (takes care of creating the topic):

#+BEGIN_SRC bash
docker exec -it schema-registry \
kafka-avro-console-producer --broker-list broker:29092 --topic events-stream --property value.schema='{"type":"record","name":"myrecord","fields":[{"name":"f1","type":"string"}]}'
#+END_SRC

Publish some message to the topic:

#+BEGIN_SRC bash
{"f1": "value1"}
{"f1": "value2"}
{"f1": "value3"}
{"f1": "value4"}
{"f1": "value5"}
{"f1": "value6"}
{"f1": "value7"}
{"f1": "value8"}
{"f1": "value9"}
#+END_SRC

* IN-PROGRESS S3 sink connector

Start worker (see https://docs.confluent.io/kafka-connect-s3-sink/current/index.html#basic-example for the explanation of config values):

#+BEGIN_SRC bash
curl -X POST \
-H "Content-Type: application/json" \
--data '{ "name": "s3-sink", "config": { "connector.class": "io.confluent.connect.s3.S3SinkConnector", "tasks.max": 1, "topics": "events-stream", "s3.region": "us-east-2", "s3.bucket.name": "clash-s3-sink", "s3.part.size": 5242880, "flush.size": 10000, "storage.class": "io.confluent.connect.s3.storage.S3Storage", "format.class": "io.confluent.connect.s3.format.avro.AvroFormat", "schema.generator.class": "io.confluent.connect.storage.hive.schema.DefaultSchemaGenerator", "partitioner.class": "io.confluent.connect.storage.partitioner.TimeBasedPartitioner",  "schema.compatibility": "NONE", "partition.duration.ms": 2000, "path.format": "YYYY/M/d/h", "locale": "US", "timezone": "UTC", "rotate.schedule.interval.ms": 60000 } }' \
http://localhost:8083/connectors
#+END_SRC
